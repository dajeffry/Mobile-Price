---
title: "Mobile Price Classification"
author: "Daniel Jeffry"
date: "4/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description

One of the companies wants to started his own mobile company. they wants to give tough fight to big companies like Apple, Samsung, etc. They does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem they collects sales data of mobile phones of various companies.  

This report describe moblie price classification using Mchine Learning Algorithm. We investigated 4 Algorithm : Logistic Regression, Decision Tree, Random Forest, and Support Vector Machine (SVM).  
The dataset used in this report is Mobile Price Classification hosted in kaggle.  

The dataset can be downloaded  

{here} https://www.kaggle.com/iabhishekofficial/mobile-price-classification  

**Report Outline**  
1. Data Extraction  
2. Exploratory Data Analysis  
3. Modelling  
4. Evaluation  
5. Recommendation  

## 1. Data Extraction  

The Dataset is downloaded from kaggle and saved in the data folder. We use read.csv function to read the dataset and put in mobile_train df for data train and mibile_test_df for data test.

```{r}
mobile_test_df <- read.csv("Data/test.csv")
mobile_train_df <- read.csv("Data/train.csv")
```

To see the number of rows and column names and types, we used dim function. The dataset has 2000 rows and 21 columns  

```{r}
dim(mobile_train_df)
```

## 2. Exploratory Data Analysis  

To find out the column names and types, we used **str() function

```{r}
str(mobile_train_df)
str(mobile_test_df)
```

From the result above, we know the following, The first column in data test is *id*. It is unique and unnecassary for prediction. so, it should be removed.   

To find out head of data and correlation in variables, we used **head() and **corrgram() function

```{r}
head(mobile_train_df)
head(mobile_test_df)

library(corrgram)
corrgram(mobile_train_df[6:16], order = TRUE,
         upper.panel = panel.pie)
```

We need to change data type from numeric to factor  

```{r}
mobile_train_df$price_range <- factor(mobile_train_df$price_range, levels = c(0,1,2,3),
                       labels = c("low cost", "medium cost", "high cost", "very high cost"))

mobile_train_df$blue <- factor(mobile_train_df$blue, levels = c(0,1),
                                  labels = c("not", "yes"))
mobile_train_df$dual_sim <- factor(mobile_train_df$dual_sim, levels = c(0,1),
                               labels = c("not", "yes"))
mobile_train_df$four_g <- factor(mobile_train_df$four_g, levels = c(0,1),
                               labels = c("not", "yes"))
mobile_train_df$three_g <- factor(mobile_train_df$three_g, levels = c(0,1),
                               labels = c("not", "yes"))
mobile_train_df$touch_screen <- factor(mobile_train_df$touch_screen, levels = c(0,1),
                               labels = c("not", "yes"))
mobile_train_df$wifi <- factor(mobile_train_df$wifi, levels = c(0,1),
                               labels = c("not", "yes"))
```

And also we need to change data type from integer to numeric so that we can made bivariate data analysis.  
```{r}
# change to numeric 
mobile_train_df$fc <- as.numeric(mobile_train_df$fc)
mobile_train_df$int_memory <- as.numeric(mobile_train_df$int_memory)
mobile_train_df$mobile_wt <- as.numeric(mobile_train_df$mobile_wt)
mobile_train_df$n_cores <- as.numeric(mobile_train_df$n_cores)
mobile_train_df$px_height <- as.numeric(mobile_train_df$px_height)
mobile_train_df$px_width <- as.numeric(mobile_train_df$px_width)
mobile_train_df$ram <- as.numeric(mobile_train_df$ram)
mobile_train_df$sc_h <- as.numeric(mobile_train_df$sc_h)
mobile_train_df$sc_w <- as.numeric(mobile_train_df$sc_w)
mobile_train_df$talk_time <- as.numeric(mobile_train_df$talk_time)
```


### 2.1 Univariate Data Analysis  
Analysis one variable   

```{r}
library(ggplot2)
ggplot(data=mobile_train_df, aes(x = price_range)) +
  geom_bar()
ggplot(data = mobile_train_df, aes(y=price_range)) +
  geom_boxplot() +
  labs(title = "Mobile Price Classification", y="Price Range")
```

From the result above, we know the amount of data in dataset has the same amount in each class that consist of low cost, medium cost, high cost, and very high cost.  

### 2.2 Bivariate Data Analysis  
Analysis of two variables, We can find out some relation between features of mobile phone and its selling price.  

```{r}
ggplot(mobile_train_df, aes(x=price_range, fill = blue)) +
  geom_bar(position = "dodge")
ggplot(mobile_train_df, aes(x=price_range, fill = dual_sim)) +
  geom_bar(position = "dodge")
ggplot(mobile_train_df, aes(x=price_range, fill = four_g)) +
  geom_bar(position = "dodge")
ggplot(mobile_train_df, aes(x=price_range, fill = three_g)) +
  geom_bar(position = "dodge")
ggplot(mobile_train_df, aes(x=price_range, fill = touch_screen)) +
  geom_bar(position = "dodge")
ggplot(mobile_train_df, aes(x=price_range, fill = wifi)) +
  geom_bar(position = "dodge")
```

Observations based on **ram** and **cores** variables. We need to know whether the number of ram and cores can affect the price.  
The color and shape of the observations are based on selling price (low cost, medium cost, high cost, and very high cost).  
```{r}
ggplot(data=mobile_train_df, aes(x=ram, y=n_cores,
                        shape=price_range, color=price_range)) +
  geom_point() +
  labs(title = "Mobile Price Classification", x="Ram", y="Cores")
```

In general, the number of ram and cores has a big effect on the price range. Smartphone that have a big ram are classified as having a higher price than a small ram. However, these two variables are not enough to separate the classes.  

## 4. Modelling  

We use 4 Machine Learning Algorithms.  

### 4.1 Logistic Regression  

```{r, massage=FALSE}
fit.logit <- glm(price_range~. ,
                 data = mobile_train_df,
                 family = binomial)
summary(fit.logit)
```

### 4.2 Decision Tree  

```{r, massage=FALSE}
library(party)
fit.ctree <- ctree(price_range~. , data = mobile_train_df)
plot(fit.ctree, main = "Conditional Inference Tree")
```

### 4.3 Random Forest  

```{r, massage=FALSE}
library(randomForest)

set.seed(2021)
fit.forest <- randomForest(formula = price_range ~ ., data = mobile_train_df,
                           na.action = na.roughfix,
                           importance = TRUE)
fit.forest
```

### 4.4 Support Vector Machine (SVM)  

```{r, massage=FALSE}
library(e1071)
set.seed(2021)
fit.svm <- svm(price_range~., data=mobile_train_df)
fit.svm
```





